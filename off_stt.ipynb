{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vosk\n",
    "!pip install sounddevice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening... Press Ctrl+C to stop\n",
      "Recognized Text: mommy i this if i have heard the healthy meal settlement with it\n",
      "Recognized Text: can you hear me\n",
      "Recognized Text: okay ours is it's very a light or something\n",
      "Recognized Text: how k\n",
      "Recognized Text: stop\n",
      "Recognized Text: how this going on\n",
      "Recognized Text: hey\n",
      "Recognized Text: how to solve this\n",
      "\n",
      "Stopping speech recognition...\n"
     ]
    }
   ],
   "source": [
    "import vosk\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import json\n",
    "import queue\n",
    "import threading\n",
    "import sys\n",
    "import scipy statistics\n",
    "class RealTimeSpeechRecognition:\n",
    "    def __init__(self, model_path, sample_rate=16000, channels=1):\n",
    "        try:\n",
    "            # Initialize Vosk model\n",
    "            self.model = vosk.Model(model_path)\n",
    "            self.sample_rate = sample_rate\n",
    "            self.channels = channels\n",
    "            \n",
    "            # Create recognizer instance\n",
    "            self.recognizer = vosk.KaldiRecognizer(self.model, sample_rate)\n",
    "            \n",
    "            # Queue for thread-safe processing\n",
    "            self.audio_queue = queue.Queue()\n",
    "            \n",
    "            # Flag to control recognition thread\n",
    "            self.is_running = threading.Event()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing speech recognition: {e}\")\n",
    "            sys.exit(1)\n",
    "    \n",
    "    def audio_callback(self, indata, frames, time, status):\n",
    "        \"\"\"\n",
    "        Callback function to process audio stream\n",
    "        \"\"\"\n",
    "        if status:\n",
    "            print(status)\n",
    "        \n",
    "        # Convert numpy array to bytes\n",
    "        audio_bytes = indata.astype(np.int16).tobytes()\n",
    "        self.audio_queue.put(audio_bytes)\n",
    "    \n",
    "    def recognition_thread(self):\n",
    "        \"\"\"\n",
    "        Separate thread for continuous speech recognition\n",
    "        \"\"\"\n",
    "        while self.is_running.is_set():\n",
    "            try:\n",
    "                # Get audio data from queue\n",
    "                audio_bytes = self.audio_queue.get(timeout=1)\n",
    "                \n",
    "                # Recognize speech\n",
    "                if self.recognizer.AcceptWaveform(audio_bytes):\n",
    "                    result = json.loads(self.recognizer.Result())\n",
    "                    text = result.get('text', '').strip()\n",
    "                    \n",
    "                    if text:\n",
    "                        print(\"Recognized Text:\", text)\n",
    "            \n",
    "            except queue.Empty:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Recognition error: {e}\")\n",
    "    \n",
    "    def start_recognition(self):\n",
    "        \"\"\"\n",
    "        Start real-time speech recognition\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Set running flag\n",
    "            self.is_running.set()\n",
    "            \n",
    "            # Start recognition thread\n",
    "            rec_thread = threading.Thread(target=self.recognition_thread)\n",
    "            rec_thread.daemon = True\n",
    "            rec_thread.start()\n",
    "            \n",
    "            # Start audio input stream\n",
    "            with sd.InputStream(\n",
    "                callback=self.audio_callback, \n",
    "                channels=self.channels, \n",
    "                samplerate=self.sample_rate, \n",
    "                dtype='int16'\n",
    "            ):\n",
    "                print(\"Listening... Press Ctrl+C to stop\")\n",
    "                \n",
    "                # Keep main thread running\n",
    "                while self.is_running.is_set():\n",
    "                    sd.sleep(100)\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nStopping speech recognition...\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during recognition: {e}\")\n",
    "        finally:\n",
    "            # Stop recognition\n",
    "            self.is_running.clear()\n",
    "\n",
    "def main():\n",
    "    # Specify your model path\n",
    "    MODEL_PATH = \"F:\\\\offline_stt\\\\vosk-model-small-en-us-0.15\\\\vosk-model-small-en-us-0.15\"\n",
    "    \n",
    "    # Create and start recognition\n",
    "    stt = RealTimeSpeechRecognition(MODEL_PATH)\n",
    "    stt.start_recognition()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test1.wav'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 176\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 176\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 162\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    160\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m case \u001b[38;5;129;01min\u001b[39;00m test_cases:\n\u001b[1;32m--> 162\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_recognition\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mcase\u001b[39;49;00m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maudio_file\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mcase\u001b[39;49;00m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mground_truth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Result: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 30\u001b[0m, in \u001b[0;36mSTTEvaluator.test_recognition\u001b[1;34m(self, audio_file, ground_truth)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03mEvaluate speech recognition on a specific audio file\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03mdict: Evaluation metrics\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Load audio file\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m audio_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Start timing\u001b[39;00m\n\u001b[0;32m     33\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[1;32mIn[12], line 73\u001b[0m, in \u001b[0;36mSTTEvaluator._load_audio\u001b[1;34m(self, file_path, dtype)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03mLoad audio file\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03mnumpy.ndarray: Audio data\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wavfile\n\u001b[1;32m---> 73\u001b[0m sample_rate, audio \u001b[38;5;241m=\u001b[39m \u001b[43mwavfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m audio\u001b[38;5;241m.\u001b[39mastype(dtype)\n",
      "File \u001b[1;32mf:\\offline_stt\\env\\Lib\\site-packages\\scipy\\io\\wavfile.py:674\u001b[0m, in \u001b[0;36mread\u001b[1;34m(filename, mmap)\u001b[0m\n\u001b[0;32m    672\u001b[0m     mmap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 674\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    677\u001b[0m     file_size, is_big_endian, is_rf64 \u001b[38;5;241m=\u001b[39m _read_riff_chunk(fid)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test1.wav'"
     ]
    }
   ],
   "source": [
    "import vosk\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import statistics\n",
    "\n",
    "class STTEvaluator:\n",
    "    def __init__(self, model_path, sample_rate=16000):\n",
    "        # Initialize Vosk model\n",
    "        self.model = vosk.Model(model_path)\n",
    "        self.recognizer = vosk.KaldiRecognizer(self.model, sample_rate)\n",
    "        \n",
    "        # Evaluation metrics\n",
    "        self.recognition_times = []\n",
    "        self.accuracy_scores = []\n",
    "    \n",
    "    def test_recognition(self, audio_file, ground_truth):\n",
    "        \"\"\"\n",
    "        Evaluate speech recognition on a specific audio file\n",
    "        \n",
    "        Args:\n",
    "        audio_file (str): Path to audio file\n",
    "        ground_truth (str): Correct transcription\n",
    "        \n",
    "        Returns:\n",
    "        dict: Evaluation metrics\n",
    "        \"\"\"\n",
    "        # Load audio file\n",
    "        audio_data = self._load_audio(audio_file)\n",
    "        \n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Recognize speech\n",
    "        results = []\n",
    "        for chunk in self._audio_chunks(audio_data):\n",
    "            if self.recognizer.AcceptWaveform(chunk):\n",
    "                result = json.loads(self.recognizer.Result())\n",
    "                results.append(result['text'])\n",
    "        \n",
    "        # Final result\n",
    "        final_result = ' '.join(results)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        recognition_time = time.time() - start_time\n",
    "        wer = self._calculate_wer(ground_truth, final_result)\n",
    "        \n",
    "        # Store metrics\n",
    "        self.recognition_times.append(recognition_time)\n",
    "        self.accuracy_scores.append(1 - wer)\n",
    "        \n",
    "        return {\n",
    "            'recognized_text': final_result,\n",
    "            'ground_truth': ground_truth,\n",
    "            'recognition_time': recognition_time,\n",
    "            'word_error_rate': wer,\n",
    "            'accuracy': 1 - wer\n",
    "        }\n",
    "    \n",
    "    def _load_audio(self, file_path, dtype=np.int16):\n",
    "        \"\"\"\n",
    "        Load audio file\n",
    "        \n",
    "        Args:\n",
    "        file_path (str): Path to audio file\n",
    "        dtype (numpy dtype): Data type for audio\n",
    "        \n",
    "        Returns:\n",
    "        numpy.ndarray: Audio data\n",
    "        \"\"\"\n",
    "        from scipy.io import wavfile\n",
    "        sample_rate, audio = wavfile.read(file_path)\n",
    "        return audio.astype(dtype)\n",
    "    \n",
    "    def _audio_chunks(self, audio_data, chunk_size=16000):\n",
    "        \"\"\"\n",
    "        Split audio into processable chunks\n",
    "        \n",
    "        Args:\n",
    "        audio_data (numpy.ndarray): Full audio data\n",
    "        chunk_size (int): Size of each chunk\n",
    "        \n",
    "        Yields:\n",
    "        bytes: Audio chunks\n",
    "        \"\"\"\n",
    "        for i in range(0, len(audio_data), chunk_size):\n",
    "            chunk = audio_data[i:i+chunk_size]\n",
    "            yield chunk.tobytes()\n",
    "    \n",
    "    def _calculate_wer(self, ground_truth, recognized_text):\n",
    "        \"\"\"\n",
    "        Calculate Word Error Rate\n",
    "        \n",
    "        Args:\n",
    "        ground_truth (str): Correct text\n",
    "        recognized_text (str): Recognized text\n",
    "        \n",
    "        Returns:\n",
    "        float: Word Error Rate\n",
    "        \"\"\"\n",
    "        ground_words = ground_truth.lower().split()\n",
    "        recognized_words = recognized_text.lower().split()\n",
    "        \n",
    "        def levenshtein_distance(s1, s2):\n",
    "            m, n = len(s1), len(s2)\n",
    "            dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "            \n",
    "            for i in range(m + 1):\n",
    "                dp[i][0] = i\n",
    "            for j in range(n + 1):\n",
    "                dp[0][j] = j\n",
    "            \n",
    "            for i in range(1, m + 1):\n",
    "                for j in range(1, n + 1):\n",
    "                    if s1[i-1] == s2[j-1]:\n",
    "                        dp[i][j] = dp[i-1][j-1]\n",
    "                    else:\n",
    "                        dp[i][j] = 1 + min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])\n",
    "            \n",
    "            return dp[m][n]\n",
    "        \n",
    "        edit_distance = levenshtein_distance(ground_words, recognized_words)\n",
    "        return edit_distance / len(ground_words)\n",
    "    \n",
    "    def get_overall_performance(self):\n",
    "        \"\"\"\n",
    "        Get overall performance metrics\n",
    "        \n",
    "        Returns:\n",
    "        dict: Overall evaluation metrics\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'average_recognition_time': statistics.mean(self.recognition_times),\n",
    "            'average_accuracy': statistics.mean(self.accuracy_scores),\n",
    "            'recognition_time_std': statistics.stdev(self.recognition_times),\n",
    "            'accuracy_std': statistics.stdev(self.accuracy_scores)\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    MODEL_PATH = \"F:\\\\offline_stt\\\\vosk-model-small-en-us-0.15\\\\vosk-model-small-en-us-0.15\"\n",
    "    \n",
    "    # Test audio files and ground truths\n",
    "    test_cases = [\n",
    "        {\n",
    "            'audio_file': 'test1.wav',\n",
    "            'ground_truth': 'hello world how are you'\n",
    "        },\n",
    "        {\n",
    "            'audio_file': 'test2.wav',\n",
    "            'ground_truth': 'python speech recognition is working'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    evaluator = STTEvaluator(MODEL_PATH)\n",
    "    \n",
    "    # Run evaluations\n",
    "    results = []\n",
    "    for case in test_cases:\n",
    "        result = evaluator.test_recognition(\n",
    "            case['audio_file'], \n",
    "            case['ground_truth']\n",
    "        )\n",
    "        results.append(result)\n",
    "        print(f\"Test Result: {result}\")\n",
    "    \n",
    "    # Overall performance\n",
    "    performance = evaluator.get_overall_performance()\n",
    "    print(\"\\nOverall Performance:\")\n",
    "    for metric, value in performance.items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
